# ParsParaphraser

This project presents a novel Transformer-based Multi-Head Self-Attention and Seq2Seq framework tailored for Persian sentence paraphrasing. Leveraging the attention mechanism, the approach achieves high performance on paraphrasing tasks while addressing challenges in the Persian language. Our method enhances the understanding of long-range dependencies and parallelization, enabling the generation of coherent and diverse paraphrases.
